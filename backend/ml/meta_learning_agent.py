"""
Agent Meta-Learning (Agent 11)
Agent qui apprend des performances et optimise les autres agents
"""

import logging
from typing import Dict, List
from emergentintegrations.llm.chat import LlmChat, UserMessage

logger = logging.getLogger(__name__)


class MetaLearningAgent:
    """
    Agent 11 - Meta-Learning
    
    ResponsabilitÃ©s:
    - Analyser les performances des 10 autres agents
    - Identifier ce qui fonctionne et ce qui Ã©choue
    - Ajuster dynamiquement les prompts des agents
    - Apprendre des patterns de succÃ¨s
    - Ne JAMAIS rÃ©pÃ©ter les erreurs du passÃ©
    """
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.logger = logging.getLogger("Agent-MetaLearning")
        self.learned_improvements = []
        self.error_memory = []  # MÃ©moire des erreurs Ã  ne pas rÃ©pÃ©ter
    
    async def analyze_and_learn(
        self,
        ml_insights: Dict,
        recent_generations: List[Dict]
    ) -> Dict[str, str]:
        """
        Analyse les insights ML et gÃ©nÃ¨re des amÃ©liorations
        
        Args:
            ml_insights: Insights du systÃ¨me ML
            recent_generations: DerniÃ¨res gÃ©nÃ©rations pour analyse
        
        Returns:
            Dict avec recommendations d'amÃ©lioration par agent
        """
        
        self.logger.info("ğŸ§  Agent Meta-Learning: Analyse et apprentissage")
        
        chat = LlmChat(
            api_key=self.api_key,
            session_id="meta-learning-agent",
            system_message=self._get_system_message()
        )
        
        prompt = self._build_analysis_prompt(ml_insights, recent_generations)
        
        try:
            response = await chat.with_model("openai", "gpt-4o").send_message(
                UserMessage(text=prompt)
            )
            
            # Parser les recommendations
            improvements = self._parse_improvements(response)
            
            # MÃ©moriser les amÃ©liorations
            self.learned_improvements.extend(improvements.get("improvements", []))
            
            self.logger.info(f"âœ… Meta-Learning: {len(improvements)} amÃ©liorations identifiÃ©es")
            
            return improvements
            
        except Exception as e:
            self.logger.error(f"âŒ Meta-Learning erreur: {e}")
            return {}
    
    def _get_system_message(self) -> str:
        """System message pour Meta-Learning Agent"""
        
        return """Tu es l'Agent Meta-Learning (Agent 11) - Le CERVEAU du systÃ¨me.

Ton rÃ´le CRITIQUE:
1. Analyser les performances des 10 autres agents
2. Identifier patterns de SUCCÃˆS et d'Ã‰CHEC
3. Apprendre continuellement de chaque gÃ©nÃ©ration
4. Ne JAMAIS rÃ©pÃ©ter les erreurs du passÃ©
5. Optimiser les prompts et comportements des agents
6. Tendre vers la perfection (score 100/100)

Tu as accÃ¨s Ã :
- Insights ML (patterns, ratios, santÃ© des agents)
- Historique des gÃ©nÃ©rations (succÃ¨s et Ã©checs)
- Feedback utilisateurs
- Erreurs et modifications

Principes d'apprentissage:
- Si une approche Ã©choue rÃ©pÃ©titivement, l'abandonner
- Si une approche rÃ©ussit, la renforcer
- Ã‰quilibrer innovation et stabilitÃ©
- Optimiser ratios mathÃ©matiques entre agents
- Adaptation continue

Tu rÃ©ponds avec des recommendations CONCRÃˆTES et ACTIONNABLES."""
    
    def _build_analysis_prompt(
        self,
        ml_insights: Dict,
        recent_generations: List[Dict]
    ) -> str:
        """Construit le prompt d'analyse"""
        
        system_score = ml_insights.get("system_score", 0)
        success_patterns = ml_insights.get("success_patterns", {})
        failure_patterns = ml_insights.get("failure_patterns", {})
        agents_health = ml_insights.get("agents_health", {})
        optimal_ratios = ml_insights.get("optimal_ratios", {})
        
        prompt = f"""ANALYSE META-LEARNING - SystÃ¨me Vectort.io

Ã‰TAT ACTUEL:
- Score systÃ¨me: {system_score:.1f}/100 (objectif: 100/100)
- ItÃ©rations d'apprentissage: {ml_insights.get('learning_iterations', 0)}
- GÃ©nÃ©rations rÃ©centes: {len(recent_generations)}

PATTERNS DE SUCCÃˆS identifiÃ©s:
{self._format_patterns(success_patterns)}

PATTERNS D'Ã‰CHEC identifiÃ©s:
{self._format_patterns(failure_patterns)}

SANTÃ‰ DES AGENTS:
{self._format_agents_health(agents_health)}

RATIOS OPTIMAUX calculÃ©s:
{self._format_ratios(optimal_ratios)}

ERREURS Ã€ NE PLUS RÃ‰PÃ‰TER:
{self._format_error_memory()}

MISSION:
Analyse ces donnÃ©es et fournis des RECOMMENDATIONS CONCRÃˆTES pour:

1. AMÃ‰LIORER chaque agent (prompts, comportement, focus)
2. CORRIGER les erreurs rÃ©currentes
3. OPTIMISER les ratios entre agents
4. ATTEINDRE 100/100 de qualitÃ©

Format de rÃ©ponse:
AGENT: nom_agent
AMÃ‰LIORATION: Description concrÃ¨te de l'amÃ©lioration
PROMPT_ADJUSTMENT: Ajustement suggÃ©rÃ© du prompt systÃ¨me
PRIORITÃ‰: haute|moyenne|basse

RÃ©ponds pour TOUS les agents nÃ©cessitant amÃ©lioration."""
        
        return prompt
    
    def _format_patterns(self, patterns: Dict) -> str:
        """Formate les patterns pour affichage"""
        if not patterns:
            return "Aucun pattern significatif"
        
        recommendations = patterns.get("recommendations", [])
        return "\n".join(f"- {rec}" for rec in recommendations[:5])
    
    def _format_agents_health(self, health: Dict) -> str:
        """Formate la santÃ© des agents"""
        if not health:
            return "DonnÃ©es non disponibles"
        
        lines = []
        for agent_name, stats in health.items():
            status = stats.get("status", "unknown")
            quality = stats.get("avg_quality", 0)
            error_rate = stats.get("error_rate", 0)
            
            emoji = "âœ…" if status == "healthy" else "âš ï¸"
            lines.append(
                f"{emoji} {agent_name}: qualitÃ© {quality:.1f}/100, erreurs {error_rate:.2%}"
            )
        
        return "\n".join(lines)
    
    def _format_ratios(self, ratios: Dict) -> str:
        """Formate les ratios optimaux"""
        if not ratios:
            return "Ratios par dÃ©faut"
        
        return "\n".join(f"- {agent}: {ratio:.2%}" for agent, ratio in ratios.items())
    
    def _format_error_memory(self) -> str:
        """Formate la mÃ©moire des erreurs"""
        if not self.error_memory:
            return "Aucune erreur rÃ©currente mÃ©morisÃ©e"
        
        return "\n".join(f"- {error}" for error in self.error_memory[-10:])
    
    def _parse_improvements(self, response: str) -> Dict[str, List[Dict]]:
        """Parse les amÃ©liorations suggÃ©rÃ©es"""
        
        improvements = []
        
        lines = response.split('\n')
        current_improvement = {}
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('AGENT:'):
                if current_improvement:
                    improvements.append(current_improvement)
                current_improvement = {"agent": line.split(':', 1)[1].strip()}
            
            elif line.startswith('AMÃ‰LIORATION:'):
                current_improvement["improvement"] = line.split(':', 1)[1].strip()
            
            elif line.startswith('PROMPT_ADJUSTMENT:'):
                current_improvement["prompt_adjustment"] = line.split(':', 1)[1].strip()
            
            elif line.startswith('PRIORITÃ‰:'):
                current_improvement["priority"] = line.split(':', 1)[1].strip()
        
        # Ajouter le dernier
        if current_improvement:
            improvements.append(current_improvement)
        
        return {"improvements": improvements, "count": len(improvements)}
    
    def memorize_error(self, error: str):
        """MÃ©morise une erreur Ã  ne plus rÃ©pÃ©ter"""
        if error not in self.error_memory:
            self.error_memory.append(error)
            self.logger.info(f"ğŸ§  Erreur mÃ©morisÃ©e: {error[:100]}")
    
    def get_learned_improvements(self) -> List[str]:
        """Retourne les amÃ©liorations apprises"""
        return self.learned_improvements
