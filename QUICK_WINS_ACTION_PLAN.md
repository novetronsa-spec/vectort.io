# ‚ö° Vectort.io - Plan d'Action Imm√©diat (Quick Wins)

## üéØ Objectif: Impl√©menter les am√©liorations critiques en 2 semaines

---

## üìÖ SEMAINE 1: Performance & Cache

### Jour 1-2: Cache LRU In-Memory ‚úÖ PRIORIT√â MAX

**Probl√®me actuel:** Chaque g√©n√©ration appelle le LLM (co√ªt $0.02-0.10)
**Solution:** Cache en m√©moire pour prompts similaires

**Impl√©mentation:**

```python
# Ajouter √† /app/backend/server.py

from functools import lru_cache
import hashlib
import json

def generate_cache_key(description: str, framework: str, type: str) -> str:
    """Generate unique cache key for AI generation"""
    data = {
        "description": description.lower().strip(),
        "framework": framework,
        "type": type
    }
    key_string = json.dumps(data, sort_keys=True)
    return hashlib.sha256(key_string.encode()).hexdigest()

# Cache pour 1000 derni√®res g√©n√©rations
@lru_cache(maxsize=1000)
def get_cached_code(cache_key: str):
    """Get code from database cache"""
    cached = db.generated_apps.find_one(
        {"cache_key": cache_key},
        sort=[("created_at", -1)]
    )
    return cached

# Modifier la route de g√©n√©ration
@api_router.post("/projects/{project_id}/generate")
async def generate_code_with_cache(...):
    # Generate cache key
    cache_key = generate_cache_key(
        request.description,
        request.framework,
        request.type
    )
    
    # Check cache first
    cached_result = get_cached_code(cache_key)
    if cached_result and not request.force_regenerate:
        logger.info(f"Cache hit for key: {cache_key}")
        return GeneratedApp(**cached_result)
    
    # Generate with AI if not cached
    logger.info(f"Cache miss, generating with AI: {cache_key}")
    result = await generate_with_ai(request)
    
    # Save with cache key
    result_dict = result.dict()
    result_dict["cache_key"] = cache_key
    await db.generated_apps.insert_one(result_dict)
    
    return result
```

**B√©n√©fices attendus:**
- ‚úÖ R√©duction co√ªts LLM: 30-40%
- ‚úÖ Latence: 15s ‚Üí 0.5s (cache hit)
- ‚úÖ Pas d'infrastructure additionnelle

**Testing:**
```bash
# Test 1: G√©n√©rer un projet
curl -X POST /api/projects/123/generate -d '{"description": "todo app"}'

# Test 2: M√™me projet (devrait √™tre instantan√©)
curl -X POST /api/projects/123/generate -d '{"description": "todo app"}'
```

---

### Jour 3-4: Rate Limiting ‚úÖ S√âCURIT√â

**Probl√®me:** Pas de protection contre abus API
**Solution:** Limiter les requ√™tes par user/IP

**Impl√©mentation:**

```python
# Ajouter √† requirements.txt
slowapi==0.1.9

# Ajouter √† server.py
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# Appliquer aux routes sensibles
@api_router.post("/projects/{project_id}/generate")
@limiter.limit("10/minute")  # 10 g√©n√©rations max par minute
async def generate_code(...):
    ...

@api_router.post("/auth/register")
@limiter.limit("5/hour")  # 5 inscriptions max par heure
async def register(...):
    ...

@api_router.post("/credits/purchase")
@limiter.limit("20/hour")  # Protection achat
async def purchase_credits(...):
    ...
```

**Configuration par plan:**
```python
RATE_LIMITS = {
    "free": "10/minute",
    "starter": "30/minute",
    "pro": "100/minute",
    "enterprise": "unlimited"
}

# Dynamic rate limiting
@limiter.limit(lambda: RATE_LIMITS[current_user.subscription_plan])
async def generate_code(...):
    ...
```

---

### Jour 5: Monitoring Basique ‚úÖ OBSERVABILIT√â

**Probl√®me:** Aucune visibilit√© sur les erreurs en production
**Solution:** Sentry.io pour error tracking

**Impl√©mentation:**

```python
# Ajouter √† requirements.txt
sentry-sdk[fastapi]==1.40.0

# Ajouter √† server.py (au d√©but)
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration
from sentry_sdk.integrations.starlette import StarletteIntegration

sentry_sdk.init(
    dsn=os.environ.get('SENTRY_DSN'),
    integrations=[
        StarletteIntegration(transaction_style="endpoint"),
        FastApiIntegration(transaction_style="endpoint"),
    ],
    traces_sample_rate=0.1,  # 10% des requ√™tes
    profiles_sample_rate=0.1,
    environment=os.environ.get('ENV', 'production'),
    release=f"vectort@{os.environ.get('VERSION', '1.0.0')}"
)

# Contexte utilisateur
@app.middleware("http")
async def add_sentry_context(request: Request, call_next):
    user = getattr(request.state, "user", None)
    if user:
        sentry_sdk.set_user({
            "id": user.id,
            "email": user.email,
            "username": user.full_name
        })
    response = await call_next(request)
    return response

# Custom events
def track_generation_failure(error: Exception, project_id: str):
    sentry_sdk.capture_exception(
        error,
        extras={
            "project_id": project_id,
            "error_type": type(error).__name__
        }
    )
```

**Setup Sentry:**
1. Cr√©er compte sur sentry.io (gratuit jusqu'√† 5K events/mois)
2. Copier le DSN
3. Ajouter √† .env: `SENTRY_DSN=https://xxx@sentry.io/yyy`

---

## üìÖ SEMAINE 2: Logs & Storage

### Jour 6-7: Logging Structur√© ‚úÖ DEBUGGING

**Probl√®me:** Logs d√©sorganis√©s, difficiles √† filtrer
**Solution:** JSON structured logging

**Impl√©mentation:**

```python
# Ajouter √† requirements.txt
python-json-logger==2.0.7

# Cr√©er /app/backend/utils/logger.py
import logging
from pythonjsonlogger import jsonlogger

def setup_logger():
    logger = logging.getLogger()
    
    # Handler avec format JSON
    handler = logging.StreamHandler()
    formatter = jsonlogger.JsonFormatter(
        '%(asctime)s %(name)s %(levelname)s %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    
    return logger

# Usage dans server.py
from utils.logger import setup_logger

logger = setup_logger()

# Log structured data
logger.info(
    "generation_started",
    extra={
        "user_id": user.id,
        "project_id": project_id,
        "framework": request.framework,
        "model": "gpt-5"
    }
)

logger.info(
    "generation_completed",
    extra={
        "user_id": user.id,
        "project_id": project_id,
        "duration_seconds": duration,
        "code_length": len(generated_code),
        "cost_usd": estimated_cost
    }
)

logger.error(
    "generation_failed",
    extra={
        "user_id": user.id,
        "project_id": project_id,
        "error": str(e),
        "provider": "gpt-5"
    },
    exc_info=True
)
```

**Analyse des logs:**
```bash
# Filtrer par user
cat backend.log | jq 'select(.user_id == "abc123")'

# Toutes les erreurs LLM
cat backend.log | jq 'select(.levelname == "ERROR" and .message == "generation_failed")'

# Co√ªt total journalier
cat backend.log | jq -s 'map(select(.message == "generation_completed")) | map(.cost_usd) | add'
```

---

### Jour 8-9: S3 File Storage ‚úÖ SCALABILIT√â

**Probl√®me:** Fichiers ZIP stock√©s en DB (limite MongoDB 16MB)
**Solution:** AWS S3 pour storage scalable

**Impl√©mentation:**

```python
# Ajouter √† requirements.txt
boto3==1.34.60

# Cr√©er /app/backend/utils/s3_storage.py
import boto3
import os
from datetime import datetime, timedelta

class S3Storage:
    def __init__(self):
        self.s3 = boto3.client(
            's3',
            aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),
            aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),
            region_name=os.environ.get('AWS_REGION', 'us-east-1')
        )
        self.bucket = os.environ.get('S3_BUCKET_NAME', 'vectort-generated-code')
    
    def upload_project_zip(self, project_id: str, user_id: str, zip_data: bytes) -> str:
        """Upload project ZIP to S3"""
        key = f"users/{user_id}/projects/{project_id}/code.zip"
        
        self.s3.put_object(
            Bucket=self.bucket,
            Key=key,
            Body=zip_data,
            ContentType='application/zip',
            Metadata={
                'project_id': project_id,
                'user_id': user_id,
                'generated_at': datetime.utcnow().isoformat()
            }
        )
        
        return key
    
    def generate_download_url(self, key: str, expires_in: int = 3600) -> str:
        """Generate presigned URL for download (expires in 1h)"""
        url = self.s3.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.bucket, 'Key': key},
            ExpiresIn=expires_in
        )
        return url
    
    def delete_project_files(self, user_id: str, project_id: str):
        """Delete all files for a project"""
        prefix = f"users/{user_id}/projects/{project_id}/"
        
        objects = self.s3.list_objects_v2(
            Bucket=self.bucket,
            Prefix=prefix
        )
        
        if 'Contents' in objects:
            delete_keys = [{'Key': obj['Key']} for obj in objects['Contents']]
            self.s3.delete_objects(
                Bucket=self.bucket,
                Delete={'Objects': delete_keys}
            )

s3_storage = S3Storage()

# Usage dans export routes
@api_router.post("/projects/{project_id}/export/zip")
async def export_to_zip_s3(project_id: str, current_user: User = Depends(get_current_user)):
    # Generate ZIP
    zip_data = await generate_project_zip(project_id)
    
    # Upload to S3
    s3_key = s3_storage.upload_project_zip(
        project_id=project_id,
        user_id=current_user.id,
        zip_data=zip_data
    )
    
    # Generate download URL
    download_url = s3_storage.generate_download_url(s3_key, expires_in=86400)  # 24h
    
    return {
        "download_url": download_url,
        "expires_at": datetime.utcnow() + timedelta(hours=24)
    }
```

**Configuration S3:**
```bash
# Ajouter √† .env
AWS_ACCESS_KEY_ID=AKIA...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1
S3_BUCKET_NAME=vectort-generated-code
```

**Cr√©ation bucket S3:**
```bash
# Cr√©er bucket
aws s3 mb s3://vectort-generated-code

# Configurer lifecycle policy (archive apr√®s 90 jours)
aws s3api put-bucket-lifecycle-configuration \
  --bucket vectort-generated-code \
  --lifecycle-configuration file://lifecycle.json
```

---

### Jour 10: M√©triques Prometheus ‚úÖ PERFORMANCE

**Probl√®me:** Pas de m√©triques temps r√©el
**Solution:** Prometheus + Grafana

**Impl√©mentation:**

```python
# Ajouter √† requirements.txt
prometheus-client==0.19.0
prometheus-fastapi-instrumentator==6.1.0

# Ajouter √† server.py
from prometheus_client import Counter, Histogram, Gauge
from prometheus_fastapi_instrumentator import Instrumentator

# Custom metrics
generation_counter = Counter(
    'vectort_generations_total',
    'Total number of code generations',
    ['status', 'model', 'framework']
)

generation_duration = Histogram(
    'vectort_generation_duration_seconds',
    'Time spent generating code',
    ['model', 'framework']
)

credits_balance = Gauge(
    'vectort_user_credits',
    'Current user credit balance',
    ['user_id']
)

llm_cost = Counter(
    'vectort_llm_cost_usd',
    'Total LLM API cost in USD',
    ['provider']
)

# Instrumenter FastAPI
instrumentator = Instrumentator()
instrumentator.instrument(app).expose(app, endpoint="/api/metrics")

# Usage
@api_router.post("/projects/{project_id}/generate")
async def generate_code_with_metrics(...):
    start_time = time.time()
    
    try:
        result = await generate_with_ai(request)
        
        # M√©triques de succ√®s
        generation_counter.labels(
            status='success',
            model='gpt-5',
            framework=request.framework
        ).inc()
        
        duration = time.time() - start_time
        generation_duration.labels(
            model='gpt-5',
            framework=request.framework
        ).observe(duration)
        
        return result
        
    except Exception as e:
        generation_counter.labels(
            status='error',
            model='gpt-5',
            framework=request.framework
        ).inc()
        raise
```

**Dashboard Grafana:**
```yaml
# Panels sugg√©r√©s
- Generations per minute (rate)
- Success rate (%)
- P95 generation latency
- LLM costs (cumulative)
- Active users (gauge)
- Credit consumption rate
```

---

## üß™ Testing des Quick Wins

### Test Suite Complet

```python
# /app/backend/tests/test_quick_wins.py

import pytest
from fastapi.testclient import TestClient

def test_cache_works(client: TestClient):
    """Test que le cache fonctionne"""
    # Premi√®re requ√™te
    response1 = client.post("/api/projects/123/generate", json={
        "description": "simple todo app",
        "framework": "react"
    })
    time1 = response1.elapsed.total_seconds()
    
    # Deuxi√®me requ√™te (devrait √™tre cached)
    response2 = client.post("/api/projects/123/generate", json={
        "description": "simple todo app",
        "framework": "react"
    })
    time2 = response2.elapsed.total_seconds()
    
    assert response1.json() == response2.json()
    assert time2 < time1 * 0.5  # Cache devrait √™tre 50%+ plus rapide

def test_rate_limiting(client: TestClient):
    """Test rate limiting"""
    # Faire 11 requ√™tes (limite = 10)
    responses = []
    for i in range(11):
        resp = client.post("/api/projects/123/generate", json={
            "description": f"app {i}"
        })
        responses.append(resp.status_code)
    
    # La 11√®me devrait √™tre rate limited
    assert responses[-1] == 429

def test_s3_upload(client: TestClient):
    """Test S3 storage"""
    response = client.post("/api/projects/123/export/zip")
    
    assert response.status_code == 200
    assert "download_url" in response.json()
    assert "s3.amazonaws.com" in response.json()["download_url"]
```

---

## üìä M√©triques de Succ√®s

### KPIs √† tracker apr√®s impl√©mentation:

```
Cache:
‚úÖ Cache hit rate: Target 30-50%
‚úÖ R√©duction co√ªts LLM: Target 40%
‚úÖ Latence moyenne: < 2s (vs 15s avant)

Rate Limiting:
‚úÖ Requ√™tes bloqu√©es: < 1% des requ√™tes l√©gitimes
‚úÖ Abus d√©tect√©s: Nombre de 429 errors

Monitoring:
‚úÖ Erreurs d√©tect√©es dans Sentry: 100%
‚úÖ Temps de r√©solution bugs: -50%
‚úÖ Alertes configur√©es: > 5

Storage:
‚úÖ Uploads S3: 100% succ√®s
‚úÖ Co√ªt storage: < $50/mois (d√©but)
‚úÖ Download speed: > 1MB/s
```

---

## üí∞ Co√ªt Additionnel Estim√©

```
Sentry.io:        $0-26/mois (gratuit jusqu'√† 5K events)
AWS S3:           $5-20/mois (premier TB = $0.023/GB)
Prometheus:       $0 (self-hosted)
Grafana Cloud:    $0-49/mois (gratuit jusqu'√† 10K series)
slowapi:          $0 (open source)

Total:            $5-95/mois
ROI:              √âconomies LLM > $500/mois avec cache
```

---

## ‚úÖ Checklist d'Impl√©mentation

### Semaine 1
- [ ] Jour 1-2: Impl√©menter cache LRU
  - [ ] Fonction generate_cache_key
  - [ ] Modifier route /generate
  - [ ] Tester cache hit/miss
  - [ ] Mesurer √©conomies

- [ ] Jour 3-4: Ajouter rate limiting
  - [ ] Installer slowapi
  - [ ] Configurer limites par route
  - [ ] Tester avec script
  - [ ] Documenter API limits

- [ ] Jour 5: Setup Sentry
  - [ ] Cr√©er compte Sentry
  - [ ] Configurer DSN
  - [ ] Tester error tracking
  - [ ] Cr√©er alertes

### Semaine 2
- [ ] Jour 6-7: Logging structur√©
  - [ ] Installer python-json-logger
  - [ ] Cr√©er utils/logger.py
  - [ ] Migrer tous les logs
  - [ ] Test log analysis

- [ ] Jour 8-9: S3 Storage
  - [ ] Cr√©er bucket S3
  - [ ] Impl√©menter S3Storage class
  - [ ] Migrer export ZIP vers S3
  - [ ] Tester presigned URLs

- [ ] Jour 10: Prometheus metrics
  - [ ] Installer instrumentator
  - [ ] Ajouter custom metrics
  - [ ] Exposer /metrics endpoint
  - [ ] Cr√©er dashboard Grafana

---

## üöÄ D√©ploiement

### Mise en production:

```bash
# 1. Backup database
mongodump --uri="$MONGO_URL" --out=/backup

# 2. Installer nouvelles d√©pendances
pip install -r requirements.txt

# 3. Ajouter variables d'environnement
cat >> .env << EOF
SENTRY_DSN=https://...
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
S3_BUCKET_NAME=vectort-generated-code
EOF

# 4. Restart backend
sudo supervisorctl restart backend

# 5. V√©rifier logs
tail -f /var/log/supervisor/backend.out.log

# 6. Test endpoints
curl http://localhost:8001/api/metrics
```

---

## üìñ Documentation Utilisateur

### Nouveaux Limits API:

```
G√©n√©rations par minute:
- Free:       10/min
- Starter:    30/min
- Pro:        100/min
- Enterprise: Unlimited

Note: Les requ√™tes en cache ne comptent pas vers la limite.
```

---

**Prochaine √©tape:** Phase 1 compl√®te (Celery + Redis queue)
**Estim√©:** +4-6 semaines apr√®s quick wins
**Document:** Voir `/app/ROADMAP_EVOLUTION.md` Phase 1
